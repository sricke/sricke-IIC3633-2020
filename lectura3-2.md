## Context
The paper presents a general view of the state of the art of ways to evaluate recommender systems. To do this it first starts off by comparing different types of experiments: offline, user studies and online. Some distinct differences can be made between each in terms of amount of data, time required, cost, number of measures, amongst other things. Then, it changes to analyze different metrics to rate recommending algorithms such as RMSE for CF algorithms and ranking metrics for tasks such as top-N. Finally it presents different properties unique to each Recsys that are important to analyze according to each context of implementation and ways to experiment with them. Overall it’s a very useful read as an introduction to start analyzing Recsys or to take into account for your own algorithm and how to experiment with it and improve it. 

## Ranking Analysis
It’s very hard to do a specific critic of each area covered in the reading so I will concentrate mostly on ranking metrics which are useful for top-N tasks which we already studied in the other reading of the week. In this topic I found that the authors lack different metrics to analyze but instead concentrate on only one which can be underwhelming considering the important task and the amount of literature in the topic. Just by the other readings of the week and the previous ones I can remember two other ranking metrics. The one discussed in the paper Normalized Distance based Performance Measure (NDPM) scores an algorithm comparing the order of the items that it recommends with a standard order and how much they agree on all pairs of items given that one of those items should be recommended before the other. Taking for example last week's reading (ALS algorithm) it would have been useful to compare this metric with the rank metric that calculates the weighted average of the items according to their rating and their position in the list. 

## Other critiques and ideas
Moving to the properties explained, some were explained more than others with proposed experiments and some others were just defined. For example, a lot of importance was given to the confidence metric of a Recsys, which should be, but for example, scalability that can be really important today given huge datasets, was given little attention to.

Finally, I would have liked to see more measures from a business side and not only from a user side. Taking for example, utility, this was explained mostly from the user side, this is, how much utility the recsys gave to the user but from a business side, utility can be of huge importance. This was just mentioned as analyzing how much revenue extra the company can expect to gain with the recsys compared to without but I think that this can be developed a lot further, such as the rates of conversion, new users acquired, amongst other ideas.
