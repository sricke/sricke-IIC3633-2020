## Context
Sometimes in recommender systems we don’t want to recommend a complete list of items that the user may like, but rather the best ones. This is where we need to use a top-N recommendation system, as presented in the paper. The authors make an important point that these types of systems don’t need to accurately estimate the rating that the user could give to an item, but rather just order them according to some metric. This  is a huge difference with the more common CF algorithms that base their whole training trying to reduce the error with the real rating and base their performance according to RMSE accuracy metric. The authors argue that actually these types of algorithms underperform in top-N tasks and need some adjustments to perform well.

## Testing and metric
The method to evaluate the results of different algorithms is a little bit confusing for me as to why they chose it like this. They calculate recall and precision according to the number of hits on a test set, where a hit would be having the high scored test item inside the top-N recommended list of the user. I believe this could have improvements, or rather avoid ambiguous results if they also consider the position p of the test item inside that list. As stated in the paper, the “best result corresponds to p=1 where the item is in the top of the list”, but the presented test methodology doesn’t take this into account and in this case two different algorithms with an average high and low value of p can score the same. In this sense, maybe it would be interesting to order the top-N recommended list with a confidence value, recommending the most confident items first and finding a metric that evaluates this accordingly. 
Looking at the test set, I find it interesting separating the test set between tail and head, comparing how the models change performance with items with high or low number of ratings, reducing bias towards non personalized algorithms.

## Results
In the results,  PureSVD outperforms propro RMSE trained algorithms, validating their hypothesis and the reason that they give is that the latter are trained and tested towards rated items, not considering the whole dataset.

My criticism with the results and test methodology would be that the authors present their preferred algorithm PureSVD with specific values of latent factors, but the other common algorithms with different latent factors. Some suspicion rises as to why the authors chose those values and it seems that they tuned them before the actual tests to achieve the best results compared to other algorithms, so that it validates their point beforehand. 

## Final Ideas
Some improvements I see could be analyzing how to better fill out missing values in the training set and not simply filling with zeros since this is not a very accurate prediction of reality and the algorithm could give too much importance to misleading data. Maybe they could include some type of regularization step, such as selecting at the beginning of each iteration only a certain number of these values to take part in training, and changing them in each iteration so the algorithm doesn’t overfit to them.
