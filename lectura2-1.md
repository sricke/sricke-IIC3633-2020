# Introduction
Contrary to the other papers discussed the past week, this one adapts the collaborative filtering approach to recommender systems using implicit data. This corresponds to using user data usage or behavior to estimate the rating that the user gives to the specific item, this is how much the user liked it, in order to create future recommendations. This comes specially in handy in today's world where there’s a huge amount of data per user but few of these users actually leave some type of rating of an item. 

# Summary
Where in explicit data we can talk of numerical values such as ratings, the authors change that concept in the paper to observations. These observations can be for example, number of times the user clicks on a product, time spent watching a show, etc., and this numerical value will show the confidence to which we can state that the user actually liked the item. The approach taken by the authors is to train the data using SVD to find a vector x for every user and a vector y that can factor the user preferences. Two main additions are done, first a regularization that comes in handy to avoid overfitting (something common in SVD) and secondly augmentation the data, where the observation metric (which is a real number) is adapted to a binary variable where if the observation takes a value bigger than vero it takes the value of one and in the other case cero. The authors make a valid point that this is not actually an indication if the user liked the item or not, so they introduce another variable representing the confidence, which basically increases the observation value the bigger the former is.

# Comments
My main comments to this paper would be in the former definition of variables. Where as clearly having an observation value equal to cero doesn’t mean that the user didn’t like the product, actually on the contrary, some of the recommended products can fall into this category (a tv show that the user hasn’t seen for example can be an excellent recommended product), a low observed value could mean different things. Taking a movie and time spent watching for example, the user maybe watched very little because he did’t like it or maybe had to stop watching because he didn’t have enough time. Here I think that similarity, the metric that was discussed last week could come in handy, where we can be more confident of the user preference if the observed value between similar items has a lower gap. For example, if time spent watching a film is very low, but the film is very similar to another one that the user spent a lot of time watching then we can be less confident that the user actually disliked it, and the same analogy could be made with being more confident if the user spent also little time watching similar movies. 

Finally the ranking metric that the authors present for their experiment I believe could be changed. They discuss that if users watch shows closer to the top of the recommended list it’s an indicator of a good recommendation, but this is not necessarily the case. Taking the same dataset, if the user clicked on the first recommended product but spent little time watching it, it could mean that the user didn’t like it. So the ranked metric could be negatively weighted by the time spent watching the show, so if the user spent little time watching the show the error goes up more. 
