Being the other paper discussed this week an introduction to implicit recommendation systems, this paper dives deep into the math and optimization beneath the main algorithms to solve this problem, such as KNN and MF. The authors propose a Bayesian approach which they call BPR and a generic optimization criterion and learning algorithm.

The authors make a good approach to discuss all the steps taken to deduce the formulas and leave everything clear for the reader. One of the more interesting points discussed is the BPR learning algorithm which they propose a stochastic gradient descent, but since their data matrix turns out to be so big and each entry is from a specific user and item pair, consecutives updates per each item turn out to be a big toll for the optimization algorithm and slows down convergence heavily. Their analisis to this is that each user item pair doesn’t need to check all other items to deduce a learning direction so they propose random sampling of entries that the algorithm learns from in each iteration. Repetition is very unlikely and successive steps ensure covering a large percent of the matrix. My main comments on this would be that this random sampling, as used in other applications of machine learning, works as a regularization technique so it would be interesting to see the results that this approach has without further regularization, as done in the paper. Furthermore, the paper doesn’t present a rigorous approach to find the optimal values of hyperparameters needed to tune this random stochastic optimization, such as the size of the chunk of the matrix that is taken in each learning step. 

Finally, the authors don’t discuss or explain a lot the evaluation metric, which surprisingly bases itself in AUC to evaluate prediction accuracy, even though there’s a similarity explained in the paper between it and BPR Opt, so one can expect good results. Besides this, the BPR approach to solving KNN and MF problems has impressing results in both datasets presented compared to SVD or the cosine approach. Something that would have been interesting to analyze though between these algorithms is space utilization and throughput since the data matrix presented is a triple product between a user and an item pair and can turn out to be very big in datasets.

In conclusion, the paper does a good job of explaining the optimization steps to deduce the formula and has an innovative approach to solve its convergence problem and since its more a generic method it’s very useful since it can be applied to different learning algorithms and approaches. 
